# -*- coding: utf-8 -*-
"""prediction_RNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f0B06Kwr6bwDPKCWgz3NLPMAm-XFD7ux
"""

import csv
import pandas as pd
from gensim.models import Word2Vec

csv_file = "all_tagged-3.csv"

def process_row(row, data):
    if row[3] not in data:
        data[row[3]] = [row[i] for i in range(4, 15)]
        data[row[3]].append(row[17])
        data[row[3]].append(row[20])
        # data[row[3]].append(row[18]) # this is the type of burst (P, R, ER)
        char_burst = [(row[15], row[16], row[19], row[22], row[23], row[24], row[25], row[26])]
        data[row[3]].append(char_burst)
    else:
        char_burst = (row[15], row[16], row[19], row[22], row[23], row[24], row[25], row[26])
        data[row[3]][-1].append(char_burst)
    return data

def process_csv(reader):
    people = {}
    for row in reader:
        if row[0] not in people:
            data = {}
            data = process_row(row, data)
            people[row[0]] = data
        else:
            data = people[row[0]]
            data = process_row(row, data)
            people[row[0]] = data
    return people

def read_csv(csv_file):
    with open(csv_file, "r") as f:
        reader = csv.reader(f)
        next(reader)  # skip the header
        people = process_csv(reader)
    return people

def extract_features(burst_data):
    burstStart = float(burst_data[0])
    burstDur = float(burst_data[1])
    pauseDur = float(burst_data[2])  # This is the target we want to predict
    cycleDur = float(burst_data[3])
    burstPct = float(burst_data[4])
    pausePct = float(burst_data[5])
    totalActions = int(burst_data[6])
    totalChars = int(burst_data[7])
    finalChars = int(burst_data[8])
    totalDeletions = int(burst_data[9])
    innerDeletions = int(burst_data[10])
    docLen = int(burst_data[11])

    pauses = burst_data[12]
    charBursts = burst_data[13]  # This is the list of tuples
    # print(charBursts)
    # print("##################")

    # Feature extraction from charBurst
    # avg_shift = sum(abs(int(end) - int(start)) for start, end, _ in charBursts) / len(charBursts) # not sure if good metric
    num_actions = len(charBursts)

    return {
        # 'burstStart': burstStart,
        # 'burstDur': burstDur,
        # 'cycleDur': cycleDur,
        # 'burstPct': burstPct,
        # 'pausePct': pausePct,
        'totalActions': totalActions,
        'totalChars': totalChars,
        'finalChars': finalChars,
        'totalDeletions': totalDeletions,
        'innerDeletions': innerDeletions,
        'docLen': docLen,
        # 'avg_shift': avg_shift,
        'num_actions': num_actions,
        'charBursts': charBursts,
        'pauses': pauses,
        'pauseDur': pauseDur # This is the target we want to predict
    }


def vectorise_text(df):
    charBursts = []
    poss = []
    chunkss = []
    for i in range(len(df)):
        for j in range(len(df['charBursts'][i])):

            charBurst = df['charBursts'][i][j][2]
            pos = df['charBursts'][i][j][3]
            chunks = df['charBursts'][i][j][4]
            charBursts.append(charBurst.split())
            poss.append(pos.split(","))
            chunkss.append(chunks.split(","))

    model_charBursts = Word2Vec(sentences=charBursts, vector_size=100, window=5, min_count=1, workers=4)
    model_pos = Word2Vec(sentences=poss, vector_size=100, window=5, min_count=1, workers=4)
    model_chunks = Word2Vec(sentences=chunkss, vector_size=100, window=5, min_count=1, workers=4)

    big_list_vectors_charBursts = []
    big_list_vectors_pos = []
    big_list_vectors_chunks = []
    big_list_pos_start = []
    big_list_pos_end = []
    big_list_frequencies = []
    big_list_frequencies_in_text = []
    big_list_relative_frequencies = []
    for i in range(len(df)):
        list_vectors_charBursts = []
        list_pauses = []
        list_vectors_pos = []
        list_vectors_chunks = []
        list_pos_start = []
        list_pos_end = []
        list_frequencies = []
        list_frequencies_in_text = []
        list_relative_frequencies = []
        for j in range(len(df['charBursts'][i])):
            # print(df['charBursts'][i][j])
            charBurst = df['charBursts'][i][j][2]
            pos = df['charBursts'][i][j][3]
            chunks = df['charBursts'][i][j][4]

            vectors_charBurst = [model_charBursts.wv[word] for word in charBurst.split()]
            vectors_pos = [model_pos.wv[word] for word in pos.split(",")]
            vectors_chunks = [model_chunks.wv[word] for word in chunks.split(",")]

            list_vectors_charBursts.extend(vectors_charBurst)
            list_vectors_pos.extend(vectors_pos)
            list_vectors_chunks.extend(vectors_chunks)
            list_pos_start.append(int(df['charBursts'][i][j][0]))
            list_pos_end.append(int(df['charBursts'][i][j][1]))
            list_frequencies.extend([int(f_value) if f_value != '' else 0 for f_value in df['charBursts'][i][j][5].split(",")])
            list_frequencies_in_text.extend([int(fit_value) if fit_value != '' else 0 for fit_value in df['charBursts'][i][j][6].split(",")])
            list_relative_frequencies.extend([float(rf_value) if rf_value != '' else 0 for rf_value in df['charBursts'][i][j][7].split(",")])

        big_list_vectors_charBursts.append(list_vectors_charBursts)
        big_list_vectors_pos.append(list_vectors_pos)
        big_list_vectors_chunks.append(list_vectors_chunks)
        big_list_pos_start.append(list_pos_start)
        big_list_pos_end.append(list_pos_end)
        big_list_frequencies.append(list_frequencies)
        big_list_frequencies_in_text.append(list_frequencies_in_text)
        big_list_relative_frequencies.append(list_relative_frequencies)
            # df["vectors_charBurst"]
            # df.at[i, 'charBursts'][j] = (df['charBursts'][i][j][0], df['charBursts'][i][j][1], vectors_charBurst, vectors_pos, vectors_chunks, df['charBursts'][i][j][5], df['charBursts'][i][j][6], df['charBursts'][i][j][7])

    df["vectors_charBursts"] = big_list_vectors_charBursts
    df["vectors_pos"] = big_list_vectors_pos
    df["vectors_chunks"] = big_list_vectors_chunks
    df["pos_start"] = big_list_pos_start
    df["pos_end"] = big_list_pos_end
    df["frequencies"] = big_list_frequencies
    df["frequencies_in_text"] = big_list_frequencies_in_text
    df["relative_frequencies"] = big_list_relative_frequencies
    df.drop(columns=['charBursts'], inplace=True)

def expand_arrays(df, liste, nom_de_la_colonne):
    df[nom_de_la_colonne] = liste
    expanded = df[nom_de_la_colonne].apply(pd.Series)
    double_expanded = pd.DataFrame()
    for column in expanded.columns:
        expanded_df = expanded[column].apply(pd.Series)
        expanded_df.columns = [f"{nom_de_la_colonne}_{column}_{i}" for i in range(len(expanded_df.columns))]
        double_expanded = pd.concat([double_expanded, expanded_df], axis=1)
    return double_expanded


def expand_columns(df):
    lists_pauses = []
    lists_vectors_charBursts = []
    lists_vectors_pos = []
    lists_vectors_chunks = []
    lists_pos_start = []
    lists_pos_end = []
    lists_frequencies = []
    lists_frequencies_in_text = []
    lists_relative_frequencies = []

    for i in range(len(df)):
        list_pauses = df['pauses'][i].strip("[").strip("]").split(", ")
        lists_pauses.append(list_pauses)

        list_pos_start = df['pos_start'][i]
        lists_pos_start.append(list_pos_start)

        list_pos_end = df['pos_end'][i]
        lists_pos_end.append(list_pos_end)

        list_frequencies = df['frequencies'][i]
        lists_frequencies.append(list_frequencies)

        list_frequencies_in_text = df['frequencies_in_text'][i]
        lists_frequencies_in_text.append(list_frequencies_in_text)

        list_relative_frequencies = df['relative_frequencies'][i]
        lists_relative_frequencies.append(list_relative_frequencies)

        list_vectors_charBursts = df['vectors_charBursts'][i]
        list_vectors_pos = df['vectors_pos'][i]
        list_vectors_chunks = df['vectors_chunks'][i]
        lists_vectors_charBursts.append(list_vectors_charBursts)
        lists_vectors_pos.append(list_vectors_pos)
        lists_vectors_chunks.append(list_vectors_chunks)

    df["pauses"] = lists_pauses
    pauses_expanded = df["pauses"].apply(pd.Series)
    # print(pauses_expanded.shape)
    pauses_expanded.columns = [f"pause_{i}" for i in range(len(pauses_expanded.columns))]

    df["pos_start"] = lists_pos_start
    pos_start_expanded = df["pos_start"].apply(pd.Series)
    pos_start_expanded.columns = [f"pos_start_{i}" for i in range(len(pos_start_expanded.columns))]

    df["pos_end"] = lists_pos_end
    pos_end_expanded = df["pos_end"].apply(pd.Series)
    pos_end_expanded.columns = [f"pos_end_{i}" for i in range(len(pos_end_expanded.columns))]

    df["frequencies"] = lists_frequencies
    frequencies_expanded = df["frequencies"].apply(pd.Series)
    frequencies_expanded.columns = [f"frequency_{i}" for i in range(len(frequencies_expanded.columns))]

    df["frequencies_in_text"] = lists_frequencies_in_text
    frequencies_in_text_expanded = df["frequencies_in_text"].apply(pd.Series)
    frequencies_in_text_expanded.columns = [f"frequency_in_text_{i}" for i in range(len(frequencies_in_text_expanded.columns))]

    df["relative_frequencies"] = lists_relative_frequencies
    relative_frequencies_expanded = df["relative_frequencies"].apply(pd.Series)
    relative_frequencies_expanded.columns = [f"relative_frequency_{i}" for i in range(len(relative_frequencies_expanded.columns))]

    vectors_charBursts_double_expanded = expand_arrays(df, lists_vectors_charBursts, "vectors_charBursts")
    vectors_pos_double_expanded = expand_arrays(df, lists_vectors_pos, "vectors_pos")
    vectors_chunks_double_expanded = expand_arrays(df, lists_vectors_chunks, "vectors_chunk")

    df_expanded = pd.concat([df.drop(columns=["pauses", "pos_start", "pos_end", "frequencies", "frequencies_in_text", "relative_frequencies", "vectors_charBursts", "vectors_pos", "vectors_chunks"]), pauses_expanded, pos_start_expanded, pos_end_expanded, frequencies_expanded, frequencies_in_text_expanded, relative_frequencies_expanded, vectors_charBursts_double_expanded, vectors_pos_double_expanded, vectors_chunks_double_expanded], axis=1)
    return df_expanded

def main():
    people = read_csv(csv_file)
    dataset = []
    for person in people:
        for burst_id in people[person]:
            features = extract_features(people[person][burst_id])
            dataset.append(features)
    df = pd.DataFrame(dataset)
    # print(df.head())
    vectorise_text(df)
    # df.to_csv('vectorised_data.csv')
    # with pd.option_context('display.max_rows', None, 'display.max_columns', None):
    #     print(df['charBursts'])
    df_expanded = expand_columns(df)
    # df_expanded["frequency_0"].to_csv('frequency_0.csv')
    df_expanded.to_pickle('expanded_data.pkl')
    return df_expanded

if __name__ == "__main__":
    main()

import numpy as np
import tensorflow as tf
import pandas as pd
from keras import layers
from keras.models import Model
from keras.layers import Dense, Masking, LSTM, Input
from sklearn.model_selection import train_test_split

def padding(df):
    df_numeric = df.apply(pd.to_numeric, errors='coerce')
    df_numeric = df_numeric.fillna(-9999.0)
    padded_df = tf.keras.preprocessing.sequence.pad_sequences(df_numeric.values, padding='post', value=-9999.0, dtype='float32')
    padded_df = pd.DataFrame(padded_df)
    return padded_df

df = df = pd.read_pickle('expanded_data.pkl')
# print(df.head())

X = df.drop(columns=['pauseDur'])
y = df['pauseDur']

X_pauses = X.filter(like='pause')
padded_X_pauses = padding(X_pauses)

X_charBurst = X.filter(like='charBurst')
padded_X_charBurst = padding(X_charBurst)

X_pos = X.filter(like='vectors_pos')
padded_X_pos = padding(X_pos)

X_chunks = X.filter(like='vectors_chunk')
padded_X_chunks = padding(X_chunks)

X_pos_start = X.filter(like='pos_start')
padded_X_pos_start = padding(X_pos_start)

X_pos_end = X.filter(like='pos_end')
padded_X_pos_end = padding(X_pos_end)

X_frequencies = X.filter(like='frequency_').filter(regex='^frequency_[^in_text]')
padded_X_frequencies = padding(X_frequencies)

X_frequencies_in_text = X.filter(like='frequency_in_text')
padded_X_frequencies_in_text = padding(X_frequencies_in_text)

X_relative_frequencies = X.filter(like='relative_frequency_')
padded_X_relative_frequencies = padding(X_relative_frequencies)

columns_to_drop = (
    list(X.filter(like='pause').columns) +
    list(X.filter(like='charBurst').columns) +
    list(X.filter(like='vectors_pos').columns) +
    list(X.filter(like='vectors_chunk').columns) +
    list(X.filter(like='pos_start').columns) +
    list(X.filter(like='pos_end').columns) +
    list(X.filter(like='frequency_').filter(regex='^frequency_[^in_text]').columns) +
    list(X.filter(like='frequency_in_text').columns) +
    list(X.filter(like='relative_frequency_').columns)
)

X_aggregate = X.drop(columns=columns_to_drop)

input_pauses = Input(shape=(padded_X_pauses.shape[1], 1))
input_charBurst = Input(shape=(padded_X_charBurst.shape[1], 1))
input_pos = Input(shape=(padded_X_pos.shape[1], 1))
input_chunks = Input(shape=(padded_X_chunks.shape[1], 1))
input_pos_start = Input(shape=(padded_X_pos_start.shape[1], 1))
input_pos_end = Input(shape=(padded_X_pos_end.shape[1], 1))
input_frequencies = Input(shape=(padded_X_frequencies.shape[1], 1))
input_frequencies_in_text = Input(shape=(padded_X_frequencies_in_text.shape[1], 1))
input_relative_frequencies = Input(shape=(padded_X_relative_frequencies.shape[1], 1))
input_aggregate = Input(shape=(X_aggregate.shape[1],))


# pauses_array = padded_X_pauses.values
# pauses_sequence = pauses_array.reshape(pauses_array.shape[0], pauses_array.shape[1], 1)
# charBurst_array = padded_X_charBurst.values
# charBurst_sequence = charBurst_array.reshape(charBurst_array.shape[0], charBurst_array.shape[1], 1)
# pos_array = padded_X_pos.values
# pos_sequence = pos_array.reshape(pos_array.shape[0], pos_array.shape[1], 1)
# chunks_array = padded_X_chunks.values
# chunks_sequence = chunks_array.reshape(chunks_array.shape[0], chunks_array.shape[1], 1)
# pos_start_array = padded_X_pos_start.values
# pos_start_sequence = pos_start_array.reshape(pos_start_array.shape[0], pos_start_array.shape[1], 1)
# pos_end_array = padded_X_pos_end.values
# pos_end_sequence = pos_end_array.reshape(pos_end_array.shape[0], pos_end_array.shape[1], 1)
# frequencies_array = padded_X_frequencies.values
# frequencies_sequence = frequencies_array.reshape(frequencies_array.shape[0], frequencies_array.shape[1], 1)
# frequencies_in_text_array = padded_X_frequencies_in_text.values
# frequencies_in_text_sequence = frequencies_in_text_array.reshape(frequencies_in_text_array.shape[0], frequencies_in_text_array.shape[1], 1)
# relative_frequencies_array = padded_X_relative_frequencies.values
# relative_frequencies_sequence = relative_frequencies_array.reshape(relative_frequencies_array.shape[0], relative_frequencies_array.shape[1], 1)

masked_pauses = layers.Masking(mask_value=-9999.0)(input_pauses)
masked_charBurst = layers.Masking(mask_value=-9999.0)(input_charBurst)
masked_pos = layers.Masking(mask_value=-9999.0)(input_pos)
masked_chunks = layers.Masking(mask_value=-9999.0)(input_chunks)
masked_pos_start = layers.Masking(mask_value=-9999.0)(input_pos_start)
masked_pos_end = layers.Masking(mask_value=-9999.0)(input_pos_end)
masked_frequencies = layers.Masking(mask_value=-9999.0)(input_frequencies)
masked_frequencies_in_text = layers.Masking(mask_value=-9999.0)(input_frequencies_in_text)
masked_relative_frequencies = layers.Masking(mask_value=-9999.0)(input_relative_frequencies)


lstm_pauses = LSTM(128,use_cudnn=False)(masked_pauses)
lstm_charBurst = LSTM(128,use_cudnn=False)(masked_charBurst)
lstm_pos = LSTM(128,use_cudnn=False)(masked_pos)
lstm_chunks = LSTM(128,use_cudnn=False)(masked_chunks)
lstm_pos_start = LSTM(128,use_cudnn=False)(masked_pos_start)
lstm_pos_end = LSTM(128,use_cudnn=False)(masked_pos_end)
lstm_frequencies = LSTM(128,use_cudnn=False)(masked_frequencies)
lstm_frequencies_in_text = LSTM(128,use_cudnn=False)(masked_frequencies_in_text)
lstm_relative_frequencies = LSTM(128,use_cudnn=False)(masked_relative_frequencies)

# aggregate_sequence = X_aggregate.values.reshape(X_aggregate.shape[0], X_aggregate.shape[1],)
dense_aggregate = Dense(128, activation='relu')(input_aggregate)

concatenated = layers.concatenate([lstm_pauses, lstm_charBurst, lstm_pos, lstm_chunks, lstm_pos_start, lstm_pos_end, lstm_frequencies, lstm_frequencies_in_text, dense_aggregate])

dense_out = Dense(128, activation='relu')(concatenated)
output = Dense(1, activation='linear')(dense_out)

model = Model(inputs=[input_pauses, input_charBurst, input_pos, input_chunks, input_pos_start, input_pos_end, input_frequencies, input_frequencies_in_text, input_relative_frequencies, input_aggregate], outputs=output)
model.compile(optimizer='adam', loss='mse', metrics=['mae'])
model.summary()

# split padded data into test and train
X_train_pauses, X_test_pauses = train_test_split(padded_X_pauses, test_size=0.2, random_state=42)
X_train_charBurst, X_test_charBurst = train_test_split(padded_X_charBurst, test_size=0.2, random_state=42)
X_train_pos, X_test_pos = train_test_split(padded_X_pos, test_size=0.2, random_state=42)
X_train_chunks, X_test_chunks = train_test_split(padded_X_chunks, test_size=0.2, random_state=42)
X_train_pos_start, X_test_pos_start = train_test_split(padded_X_pos_start, test_size=0.2, random_state=42)
X_train_pos_end, X_test_pos_end = train_test_split(padded_X_pos_end, test_size=0.2, random_state=42)
X_train_frequencies, X_test_frequencies = train_test_split(padded_X_frequencies, test_size=0.2, random_state=42)
X_train_frequencies_in_text, X_test_frequencies_in_text = train_test_split(padded_X_frequencies_in_text, test_size=0.2, random_state=42)
X_train_relative_frequencies, X_test_relative_frequencies = train_test_split(padded_X_relative_frequencies, test_size=0.2, random_state=42)
X_train_aggregate, X_test_aggregate = train_test_split(X_aggregate, test_size=0.2, random_state=42)

# split target into test and train
y_train, y_test = train_test_split(y, test_size=0.2, random_state=42)

model.fit([X_train_pauses, X_train_charBurst, X_train_pos, X_train_chunks, X_train_pos_start, X_train_pos_end, X_train_frequencies, X_train_frequencies_in_text, X_train_relative_frequencies, X_train_aggregate], y_train, epochs=10, batch_size=32, verbose=2, validation_split=0.2)
model.evaluate([X_test_pauses, X_test_charBurst, X_test_pos, X_test_chunks, X_test_pos_start, X_test_pos_end, X_test_frequencies, X_test_frequencies_in_text, X_test_relative_frequencies, X_test_aggregate], y_test, verbose=2)
#embedding = layers.Embedding(input_dim=116517, output_dim=128, mask_zero=True)
#masked_output = embedding(padded_X)
# masking_layer = layers.Masking()

#model = Sequential()
#model.add(Masking(mask_value=-1, input_shape=(padded_X.shape[1],)))